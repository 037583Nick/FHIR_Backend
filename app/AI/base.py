from io import BytesIO
from PIL import Image
import base64
import numpy as np
import requests
import os
import grpc

# ä½¿ç”¨æ–°ç‰ˆ tritonclient (èˆŠç‰ˆèˆ‡æ–°ç‰ˆ Triton Server ä¸å…¼å®¹)
try:
    import tritonclient.grpc as grpcclient
    from tritonclient.utils import np_to_triton_dtype
    NEW_CLIENT = True
    # print("âœ… ä½¿ç”¨æ–°ç‰ˆ tritonclient")
except ImportError:
    raise ImportError("âŒ éœ€è¦å®‰è£ tritonclientï¼")

def model_dtype_to_np(model_dtype):
    """è½‰æ›æ¨¡å‹è³‡æ–™é¡å‹åˆ° numpy é¡å‹"""
    # æ–°ç‰ˆ tritonclient ä½¿ç”¨å­—ä¸²è¡¨ç¤ºé¡å‹
    dtype_map = {
        'BOOL': np.bool,
        'INT8': np.int8,
        'INT16': np.int16,
        'INT32': np.int32,
        'INT64': np.int64,
        'UINT8': np.uint8,
        'UINT16': np.uint16,
        'FP16': np.float16,
        'FP32': np.float32,
        'FP64': np.float64,
        'BYTES': np.dtype(object)
    }
    return dtype_map.get(model_dtype, None)

class ModelNotReadyException(Exception):
    pass

class ModernBasePreprocessor:
    """ä½¿ç”¨æ–°ç‰ˆ tritonclient çš„åŸºç¤é è™•ç†å™¨"""
    
    def __init__(self, fn, model_name, server=None, torch=False, model_ver="1.0"):
        self.fn = fn
        self.image = self.load_image(fn)
        self.model_name = model_name
        self.output_list = []
        self.torch = torch
        self.model_ver = model_ver
        
        # å¾ç’°å¢ƒè®Šæ•¸è®€å– gRPC ä¼ºæœå™¨åœ°å€
        if server is None:
            server = os.getenv("GRPC_SERVER_ADDRESS", "10.69.12.83:8006")
        self.server = server
        
        if not torch:
            self._init_new_client()
    
    def _init_new_client(self):
        """åˆå§‹åŒ–æ–°ç‰ˆ tritonclient"""
        try:
            self.grpc_client = grpcclient.InferenceServerClient(
                url=self.server,
                verbose=False
            )
            
            # æª¢æŸ¥æœå‹™å™¨æ˜¯å¦å°±ç·’
            if not self.grpc_client.is_server_ready():
                raise Exception("Triton server is not ready")
            
            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å°±ç·’
            if not self.grpc_client.is_model_ready(self.model_name):
                raise ModelNotReadyException(f"Model {self.model_name} is not ready")
            
            # ç²å–æ¨¡å‹é…ç½®
            self.model_config = self.grpc_client.get_model_config(self.model_name)
            # print(f"âœ… æ¨¡å‹ {self.model_name} å·²å°±ç·’")
            
        except Exception as e:
            print(f"âŒ æ–°ç‰ˆå®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def get_image(self):
        return self.image

    def load_image(self, fn):
        # é€™å€‹æ–¹æ³•éœ€è¦åœ¨å­é¡ä¸­å¯¦ç¾
        raise NotImplementedError("load_image must be implemented by subclass")

    def infer_one(self, input_dataset):
        """æ¨è«–å–®å€‹æ•¸æ“šé›†ï¼Œå®Œå…¨æ¨¡ä»¿èˆŠç‰ˆé‚è¼¯"""
        if self.torch:
            # PyTorch æœå‹™å™¨æ¨è«–
            byte_io = BytesIO()
            np.save(byte_io, input_dataset)
            url = f"http://{self.server}/predictions/{self.model_name}/{self.model_ver}/"
            r = requests.post(url, data=byte_io.getvalue())
            return r.json()
        
        # ä½¿ç”¨æ–°ç‰ˆ tritonclient
        return self._infer_one_new_client_compat(input_dataset)
    
    def _infer_one_new_client_compat(self, input_dataset):
        """ä½¿ç”¨æ–°ç‰ˆ tritonclientï¼Œä½†å®Œå…¨æ¨¡ä»¿èˆŠç‰ˆçš„é‚è¼¯å’Œè¨­å®š"""
        try:
            inputs = []
            outputs = []
            
            # æ­£ç¢ºè¨ªå•æ¨¡å‹é…ç½®
            model_config = self.model_config.config
            
            # æº–å‚™è¼¸å…¥ï¼Œæ¨¡ä»¿èˆŠç‰ˆæ–¹å¼
            for input_spec, data in zip(model_config.input, input_dataset):
                # ç¢ºä¿æ‰¹æ¬¡å¤§å°ç‚º 1
                if data.ndim == 2:  # å¦‚æœæ˜¯ 2Dï¼Œæ·»åŠ æ‰¹æ¬¡ç¶­åº¦
                    data = np.expand_dims(data, axis=0)
                
                input_obj = grpcclient.InferInput(
                    input_spec.name, 
                    data.shape, 
                    np_to_triton_dtype(data.dtype)
                )
                input_obj.set_data_from_numpy(data)
                inputs.append(input_obj)

            # æº–å‚™è¼¸å‡º
            for output_spec in model_config.output:
                outputs.append(grpcclient.InferRequestedOutput(output_spec.name))

            # åŸ·è¡Œæ¨è«–
            response = self.grpc_client.infer(
                model_name=self.model_name,
                inputs=inputs,
                outputs=outputs
            )

            # æ”¶é›†çµæœï¼Œæ¨¡ä»¿èˆŠç‰ˆæ ¼å¼
            output_list = []
            for output_spec in model_config.output:
                result = response.as_numpy(output_spec.name)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æ¨™ç±¤æª”æ¡ˆï¼Œå¦‚æœæœ‰å‰‡éœ€è¦è½‰æ›æˆ (label, value) æ ¼å¼  
                if hasattr(output_spec, 'label_filename') and output_spec.label_filename:
                    # ğŸ”§ æ–°ç‰ˆ tritonclient éœ€è¦æ‰‹å‹•å¯¦ç¾èˆŠç‰ˆ trtis çš„æ¨™ç±¤è½‰æ›åŠŸèƒ½
                    # print(f"ğŸ” æ¨¡å‹ {self.model_name} æœ‰ label_filename: {output_spec.label_filename}")
                    
                    if result.ndim > 1:
                        result = result.flatten()
                    
                    # æ‰¾åˆ°æœ€é«˜æ©Ÿç‡çš„ç´¢å¼•å’Œå€¼
                    max_idx = np.argmax(result)
                    max_value = float(result[max_idx])
                    
                    # print(f"ğŸ” åŸå§‹çµæœ: æœ€é«˜æ©Ÿç‡ç´¢å¼•={max_idx}, å€¼={max_value}")
                    
                    # ğŸ”§ æ ¹æ“šæ‚¨çš„ labels.txt å…§å®¹ï¼Œæ‰‹å‹•å¯¦ç¾æ¨™ç±¤è½‰æ›
                    if self.model_name == "ecg_multicat12":
                        # å¿ƒå¾‹æ¨¡å‹ï¼šä½¿ç”¨æ‚¨æä¾›çš„ labels.txt é †åº
                        labels_from_triton = [
                            'AFIB',     # 0 - labels.txt ç¬¬1è¡Œ
                            'BIGEMINY', # 1 - labels.txt ç¬¬2è¡Œ  
                            'EAR',      # 2 - labels.txt ç¬¬3è¡Œ
                            'AFL',      # 3 - labels.txt ç¬¬4è¡Œ
                            'CHB',      # 4 - labels.txt ç¬¬5è¡Œ
                            'NSR',      # 5 - labels.txt ç¬¬6è¡Œ â† é€™å°±æ˜¯æ‚¨è¦çš„ï¼
                            'FRAV',     # 6 - labels.txt ç¬¬7è¡Œ
                            'SECAV1',   # 7 - labels.txt ç¬¬8è¡Œ
                            'VPB',      # 8 - labels.txt ç¬¬9è¡Œ
                            'APB',      # 9 - labels.txt ç¬¬10è¡Œ
                            'ST',       # 10 - labels.txt ç¬¬11è¡Œ
                            'PSVT'      # 11 - labels.txt ç¬¬12è¡Œ
                        ]
                        if max_idx < len(labels_from_triton):
                            label = labels_from_triton[max_idx]
                        else:
                            label = f'Class_{max_idx}'
                        
                        # print(f"ğŸ” æ¨™ç±¤è½‰æ›: index {max_idx} -> {label}")
                        output_list.append((label, max_value))
                    else:
                        # å…¶ä»–æ¨¡å‹ï¼ˆåŒ…æ‹¬ STEMIï¼‰ï¼šä¿æŒèˆ‡èˆŠç‰ˆä¸€è‡´çš„ (label, value) æ ¼å¼
                        if result.ndim > 1:
                            result = result.flatten()
                        max_idx = np.argmax(result)
                        max_value = float(result[max_idx])
                        
                        # ğŸ”§ ç‰¹æ®Šè™•ç†ï¼šSTEMI æ¨¡å‹ç›´æ¥ä½¿ç”¨ "STEMI" æ¨™ç±¤
                        if self.model_name == "ecg_stemi_by":
                            # STEMI äºŒå…ƒåˆ†é¡ï¼šä¸ç®¡ index æ˜¯ä»€éº¼ï¼Œéƒ½ç›´æ¥ç”¨ "STEMI" æ¨™ç±¤
                            # é€™æ¨£èˆ‡èˆŠç‰ˆä¿æŒå®Œå…¨ä¸€è‡´
                            label = "STEMI"
                            # print(f"ğŸ” STEMI æ¨¡å‹å›ºå®šæ¨™ç±¤: index {max_idx} -> {label} (å€¼: {max_value})")
                        else:
                            # å…¶ä»–æ¨¡å‹ä½¿ç”¨é€šç”¨æ¨™ç±¤
                            label = f'Class_{max_idx}'
                            # print(f"ğŸ” æ¨¡å‹ {self.model_name} é€šç”¨æ¨™ç±¤è½‰æ›: index {max_idx} -> {label}")
                        
                        output_list.append((label, max_value))
                        
                else:
                    # æ²’æœ‰æ¨™ç±¤æª”æ¡ˆï¼Œç›´æ¥å›å‚³åŸå§‹çµæœ
                    output_list.append(result)
            
            self.output_list = output_list  # è¨­å®š output_list å±¬æ€§
            return output_list
            
        except Exception as e:
            print(f"âŒ æ¨è«–å¤±æ•—: {e}")
            raise

    def get_output_list(self):
        """ç²å–è¼¸å‡ºåˆ—è¡¨ï¼Œèˆ‡èˆŠç‰ˆå…¼å®¹"""
        return getattr(self, 'output_list', [])

    def inference_new_client(self, input_data, input_name="input_1", output_name="dense_1/Sigmoid"):
        """ä½¿ç”¨æ–°ç‰ˆ tritonclient é€²è¡Œæ¨è«–"""
        try:
            # æº–å‚™è¼¸å…¥
            inputs = []
            inputs.append(grpcclient.InferInput(input_name, input_data.shape, np_to_triton_dtype(input_data.dtype)))
            inputs[0].set_data_from_numpy(input_data)

            # æº–å‚™è¼¸å‡º
            outputs = []
            outputs.append(grpcclient.InferRequestedOutput(output_name))

            # åŸ·è¡Œæ¨è«–
            response = self.grpc_client.infer(
                model_name=self.model_name,
                inputs=inputs,
                outputs=outputs
            )

            # ç²å–çµæœ
            result = response.as_numpy(output_name)
            return result
            
        except Exception as e:
            print(f"âŒ æ–°ç‰ˆå®¢æˆ¶ç«¯æ¨è«–å¤±æ•—: {e}")
            raise

    def inference_old_client(self, input_data, input_name="input_1", output_name="dense_1/Sigmoid"):
        """ä½¿ç”¨èˆŠç‰ˆ trtis å®¢æˆ¶ç«¯é€²è¡Œæ¨è«–"""
        # é€™è£¡ä¿ç•™èˆŠç‰ˆçš„æ¨è«–é‚è¼¯
        # ç”±æ–¼å¤ªè¤‡é›œï¼Œå»ºè­°ç›´æ¥ä½¿ç”¨æ–°ç‰ˆå®¢æˆ¶ç«¯
        raise NotImplementedError("è«‹å®‰è£ tritonclient ä½¿ç”¨æ–°ç‰ˆå®¢æˆ¶ç«¯")

    def inference(self, input_data, input_name="input_1", output_name="dense_1/Sigmoid"):
        """çµ±ä¸€çš„æ¨è«–æ¥å£"""
        return self.inference_new_client(input_data, input_name, output_name)

# å‘å¾Œå…¼å®¹çš„åˆ¥å
BasePreprocessor = ModernBasePreprocessor
